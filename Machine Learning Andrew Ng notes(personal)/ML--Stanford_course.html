<!doctype html><html>
    <head>
      <meta http-equiv="content-type" content="text/html; charset=utf-8">
      <title>Stanford course</title>
      <meta name="generator" content="CherryTree">
      <link rel="stylesheet" href="res/styles3.css" type="text/css" />
    </head>
    <body><div class='page'><h1 class='title'>Stanford course</h1><br/>topics covered and needs to  practiced over python<br /><a href=""><img src="images/11-1.png" alt="images/11-1.png" /></a><br /><br /><br />types of algorithms : supervised , unsupervised<br />W1-W3<br />model &amp; cost function<br />	contour plot<br />gradient descent<br />linear regression<br />batch gradient descent<br />multiple features <br />linear regression with multiple variables<br />vectorization of hypothesis function<br />gradient descent for multiple variable<br />feature scaling - gradient descent in practice I<br />	- mean normalization<br />learning rate - gradient Descent in practice II<br />Features and Polynomial Regression<br />	- Normal Equation<br />	- Normal Equation invertibility<br />W4<br />Logistic Regrssion<br />	Classification problems<br />binary classificaion , multiple classification<br />Hypothesis Representation<br />sigmoid/logistic function<br />interpretation of hypothesis output<br />decision boundary	- linear and non-linear<br />Cost function in logistic regression model<br />convex optimization<br />optimatization algorithm - gradient descent (others : conjugate descent, BFGS, L-BFGS)<br />Cost function in logistic regression using gradient descent<br />Multiclass classification<br />one vs all<br />problem of overfitting, underfitting<br />regularization - cost function<br />	- intuition<br />regularized Linear regression<br />	- gradient descent<br />	- normal equation<br />	- non-invertibility<br />regularized logistic regression<br />	- cost function, gradient descent<br />advance optimization method for regularization<br /><br />W5<br />neural Networks<br />nonlinear hypothesis<br />Non-linear classification<br />NN model representation I <br />	- logistic unit, activation/sigmoid function<br />NN model representation II<br />	- forward propagration - vectorized implementation<br />		- learning own features<br />non-linear decision boundary<br />multiclass classificaation based NN<br /><br />Classification problem in NN<br />	- cost function \\logistics regression<br />Backpropagation<br />	- gradient computation <br />	- algorithms <br />	- intuition<br />Why backpropagation<br />advance optimization <br />learning algorithm<br />gradient checking<br />random initialization : symmetry breaking<br />training NN<br /><br />W6<br />advice for applying ML<br />Deciding what to try next<br />debugging a learning algo<br />ML diagnostics<br />Evaluating hypothesis<br />Training/testing procedure for logistic regression<br />Model selection/training/validation/test sets<br />Bias &amp; Variance<br />Error<br />Diaagnosing bais vs variance<br />effect of regularization on bias/variance <br />choosing regularization parameter<br />Bias/Variance as function of regularization parameter<br />Learning curves - high bias, high variance, error<br />NN and overfitting<br /><br />Machine Learning system design<br />prioritizing what to work on : spam classifier<br />Error analysis<br />recommended approach<br />numerical evaluation of learning algo<br />handling skewed data<br />error metrics for skewed classes<br />Precison/Recall<br />Trading off precision and recall<br />Training large data sets<br />Designing a high accuracy learning system<br /><br />W7<br /><br /><br /><br /><br /><br /><br /><br />	<br /><br /></div></body></html>